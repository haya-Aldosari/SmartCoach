# -*- coding: utf-8 -*-
"""emotion_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fRUo-pz1TFkY0GBA7wqeXD9CtnXF5HhZ

# **SmartCoach** - Emotion Model Training
 ------------------------------------------------------
 Fine-tunes a pretrained ResNet18 model to classify player emotions from facial images.

 ------------------------------------------------------

**Why Fine-Tune a Pretrained Model?**

In this project, we use fine-tuning on a pretrained ResNet18 model instead of training from scratch.

This decision is based on the following scientific rationale:

Data Limitation:
Training deep neural networks from scratch requires large-scale labeled datasets. Our dataset—emotion-labeled player images—is relatively small and noisy.

Transfer Learning Advantage:
Pretrained models like ResNet18 already capture generalized visual features (edges, textures, shapes) from massive datasets like ImageNet.
Fine-tuning allows us to reuse this learned knowledge and specialize it for emotion recognition.

Faster Convergence and Lower Cost:
Since most of the network already contains relevant weights, training becomes faster and more stable, even with limited data.

Thus, we adapt only the final fully connected layer of ResNet18 to match our custom emotion classes, while keeping the rest of the model intact or partially trainable.
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, random_split
import os
import zipfile

"""# Step 1: Extract and prepare dataset
------------------------------------------------------
In this step, the dataset of facial images is extracted from a `.zip` archive and organized using the directory structure required by PyTorch's `ImageFolder.`

The archive is assumed to contain subfolders named after the emotion classes, such as: `Stress`, `Focus`, `Anger`, etc.

These subfolders are interpreted by `ImageFolder` as labeled classes, enabling automatic generation of labels for supervised training.

This step ensures the data is accessible, organized, and ready for transformation.
"""

# Unzip the dataset (contains folders named by emotion classes)

with zipfile.ZipFile('/content/labeled_faces .zip', 'r') as zip_ref:
    zip_ref.extractall('/content/labeled_faces')

# Define path to unzipped data
data_dir = '/content/labeled_faces/labeled_faces'

"""# Step 2: Define image preprocessing (augmentation + normalization)
------------------------------------------------------
Before feeding images into the neural network, we apply a series of transformations:

Resize: All images are resized to `(224x224)` pixels to match the input requirements of ResNet18.

Augmentation:

`RandomHorizontalFlip`: Introduces variation in orientation.

`ColorJitter`: Adds variability in brightness and contrast.

Normalization: Standardized using ImageNet’s mean and standard deviation to align with the pre-trained model’s expectations.

These transformations improve the generalization ability of the model by simulating real-world variability in lighting, pose, and expression.
"""

transform = transforms.Compose([
    transforms.Resize((224, 224)),                    # Resize all images to 224x224 for ResNet
    transforms.RandomHorizontalFlip(),                # Random horizontal flip (data augmentation)
    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Random changes to brightness and contrast
    transforms.ToTensor(),                            # Convert PIL image to Tensor
    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize with ImageNet standards
                         std=[0.229, 0.224, 0.225])
])

# Load dataset using torchvision's ImageFolder
dataset = datasets.ImageFolder(root=data_dir, transform=transform)

# Display detected emotion classes
print("Detected classes:", dataset.classes)
print("Number of classes:", len(dataset.classes))

"""# Step 3: Split dataset into training and validation sets
------------------------------------------------------
We divide the dataset into:

80% Training

20% Validation

This allows us to assess model performance on unseen data during training.
We then use [DataLoader](https://) to load image batches efficiently during training and evaluation, `with shuffle=True` used for training to enhance randomness and learning stability.
"""

train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

# Create DataLoaders to load batches of images
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

"""# Step 4: Prepare ResNet18 model for fine-tuning
------------------------------------------------------
In this critical stage:

We load a pre-trained ResNet18 model, which has already learned general visual features from the ImageNet dataset.

We replace the final fully connected (FC) layer with a new layer that outputs the number of emotion classes in our dataset.

This approach enables us to reuse powerful feature extraction layers, while adapting the output layer to our specific classification task (emotion detection).

We also define:

`CrossEntropyLoss` as our loss function—optimal for multi-class classification.

`Adam` as the optimizer, providing adaptive learning rates and generally fast convergence.

Finally, we move the model to the appropriate device (GPU or CPU) for efficient computation.
"""

model = models.resnet18(pretrained=True)             # Load pretrained ResNet18
num_features = model.fc.in_features
model.fc = nn.Linear(num_features, len(dataset.classes))  # Replace final FC layer to match emotion classes

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()                    # CrossEntropy for multi-class classification
optimizer = optim.Adam(model.parameters(), lr=0.001) # Adam optimizer with default settings

"""# Step 5: Train the model for defined number of epochs
------------------------------------------------------
The model is trained for a fixed number of epochs.
For each batch:

A forward pass generates predictions.

The loss is computed by comparing predictions to ground truth.

A backward pass computes gradients.

The optimizer updates the model’s weights.

Loss per epoch is printed for tracking convergence and performance.

This process gradually refines the model’s weights to improve prediction accuracy on emotion classes.
"""

epochs = 5
for epoch in range(epochs):
    model.train()                                    # Set model to training mode
    running_loss = 0.0

    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()                        # Reset gradients
        outputs = model(images)                      # Forward pass
        loss = criterion(outputs, labels)            # Compute loss
        loss.backward()                              # Backpropagation
        optimizer.step()                             # Update weights

        running_loss += loss.item()

    print(f"[{epoch+1}/{epochs}] Training loss: {running_loss / len(train_loader):.4f}")

"""# Step 6: Save the trained model for future use
------------------------------------------------------
After training, the model’s learned weights are saved in `emotion_model.pth`.
This file can later be loaded and used for:

Predicting emotions on unseen images or video frames.

Integration into larger systems, such as SmartCoach.

Saving the model allows reusability and avoids retraining every time.
"""

torch.save(model.state_dict(), 'emotion_model.pth')
print("✅ Training complete. Model saved as 'emotion_model.pth'")