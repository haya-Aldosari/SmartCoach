# -*- coding: utf-8 -*-
"""report_generator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l7gO7SHBP2wU_bfZgBVNzEMXPaJ2321Y

---------------------------------------------------------------
# SmartCoach - Emotion-Based Report Generation using Hermes-2 LLM

---------------------------------------------------------------


This notebook uses a large language model (LLM) to generate a psychological evaluation
based on previously analyzed emotional data. The emotional data is derived from either
video frames or image sequences analyzed by a fine-tuned CNN (ResNet18).

The LLM used is Hermes-2-Pro-Mistral-7B, a state-of-the-art model known for its ability
to produce detailed, coherent, and human-like text responses.
"""

# access and load open-source models from HuggingFace Hub
!pip install transformers accelerate huggingface_hub

from huggingface_hub import login
login("hf_your_token_here")  # Replace with your actual HuggingFace access token

"""# Step 2: Load the Hermes-2 LLM from HuggingFace
--------------------------------------------------
"""

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import json

# Load tokenizer and model with half-precision and auto device placement
model_name = "NousResearch/Hermes-2-Pro-Mistral-7B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.float16
)

"""# Step 3: Load Emotion Data from Previous Analysis
 --------------------------------------------------
"""

# Load JSON output produced from video or image processing
# Each entry contains a timestamp and an emotion label
with open('/content/emotions.json', 'r') as f:
    emotion_data = json.load(f)

# Convert emotion data into a readable string to pass into the LLM prompt
# Only the first 30 entries are included to stay within token limits
sequence = ", ".join([f"{e['time']}s: {e['emotion']}" for e in emotion_data[:30]])sequence = ", ".join([f"{e['time']}s: {e['emotion']}" for e in emotion_data[:30]])

"""# Step 4: Construct the LLM Prompt
--------------------------------------------------
"""

# The prompt instructs the LLM to behave as a sports psychologist
# and to generate a detailed emotional evaluation from the data
prompt = (
    "You are a professional sports psychologist. Based on the emotional data below from a football player, "
    "write a psychological evaluation describing the emotional progression, mental state, and possible insights.\n\n"
    f"Emotions:\n{sequence}\n\n"
    "Evaluation:"
)

"""# Step 5: Run Inference to Generate Report
 --------------------------------------------------
"""

# Encode prompt, generate response with controlled parameters
inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024).to(model.device)

outputs = model.generate(
    **inputs,
    max_new_tokens=300,
    do_sample=True,
    temperature=0.7,
    pad_token_id=tokenizer.eos_token_id
)

"""# Step 6: Extract and Display the Final Output
 --------------------------------------------------
"""

# Decode and isolate the modelâ€™s output from the full generated text
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
evaluation = generated_text.replace(prompt, "").strip()

print("\nðŸ§  Psychological Evaluation:\n")
print(evaluation)

